%-----------------------------------------------------------------
\begin{plSection}{Spaces}
%-----------------------------------------------------------------
\begin{plSection}{Topological spaces}
\label{sec:Topological-spaces}

A generalization of
\citeAuthorYearTitle[chapter~1]{Spivak:1965:CalculusOnManifolds}.

\textit{Open sets}, \textit{neighborhoods}.

Finite intersection of open sets is open.
Arbitrary union is is open. 

\textit{Interior}: elements in set 
with a neighborhood contained
in the set.

\textit{Exterior}: elements not in set
with a neighborhood not intersecting the set.

\textit{Boundary}: elements where every neighborhood
intersects both the interior and the exterior.

\textit{Open cover}. 

\textit{Compact} set has finite open cover.

A set is \textit{closed} if its complement is open.

Connectivity: set is connected if no disjoint open sets contain
whole set.

Limits.

\textit{Continuity} of functions: 
(See \citeAuthorYearTitle[Theorem~1-8]{Spivak:1965:CalculusOnManifolds}.)
$f \, : \, \Set{D} \mapsto \Set{C}$,
for topological spaces $\Set{D}$ and $\Set{C}$,
is \textit{continuous}
if for any open set $\Set{O}_{\Set{C}} \subset \Set{C}$,
$f^{-1}\left( \Set{O}_{\Set{C}} \right) = 
\Set{O}_{\Set{D}}$, an open set $\subset \Set{D}$.
(\textbf{TODO:} Is this assuming domain and codomain are open?
Is this generalization of Spivak 1.8 really correct?)

Example: open intervals in $\Space{R}$,
open balls in $\Space{R}^n$ with $l_1$, $l_2, l_{\infty}$ distances
(what is required to work?).
Figures for open balls with various metrics.

\end{plSection}%{Topological spaces}
%-----------------------------------------------------------------
\begin{plSection}{Metric spaces}
\label{sec:Metric-spaces}

Open sets generated by distance function.

\end{plSection}%{Metric spaces}
%-----------------------------------------------------------------
\begin{plSection}{Linear spaces}
\label{sec:Linear-spaces}

\begin{plQuote}
{\citeAuthorYearTitle{MacLane:1954:Courses}}
{maclane:vspace}
{Throughout these courses the infusion of a geometrical
point of view is of paramount importance. A vector
is geometrical; it is an element of a vector space, defined
by suitable axioms—whether the scalars be real numbers or
elements of a general field. A vector is not an n-tuple of
numbers until a coordinate system has been chosen. Any
teacher and any text book which starts with the idea that vectors
are n-tuples is committing a crime for which the proper
punishment is ridicule. The n-tuple idea is not ‘easier,’ it is
harder; it is not clearer, it is more misleading. By the same
token, linear transformations are basic and matrices are their
representations\ldots}
\end{plQuote}

My approach to linear (aka vector) spaces is largely based on
the texts I used as a college freshman for linear algebra and
multivariate calculus: Halmos \citeAuthorYearTitle{Halmos:1958:Finite}
and Spivak \citeAuthorYearTitle{Spivak:1965:CalculusOnManifolds}.

\begin{plDefinition}{Linear space}{}
A \textit{linear space} 
$\Space{V} = \left[ \Set{V}, \Space{K}, \linearCombination \right]$
 is:
\begin{itemize}
  \item a set of \textit{vectors} $\Set{V}$,
  \item a field  of scalars $\Space{K}$,
  \item a linear combination function: 
\begin{equation}
\left( \linearCombination 
\, a_0 \, \Vector{v}_0 \, a_1 \, \Vector{v}_1 \right) \; 
 \rightarrow \; \Vector{v}_2  \in \Set{V}
\end{equation}
for $\Vector{v}_0, \Vector{v}_1 \in \Set{V} $
and $a_0, a_1 \in \Space{K}$.
Linear combination is often defined in terms of
$2$ binary operations:
scalar multiplication $a * \Vector{v} \in \Set{V}$,
and vector addition $\Vector{v}_0 + \Vector{v}_1 \in \Set{V}$:
\begin{equation}
\left( \linearCombination 
\, a_0 \, \Vector{v}_0 \, a_1 \, \Vector{v}_1 \right) \; 
= \; a_0*\Vector{v}_0 + a_1*\Vector{v}_1
\end{equation}
\textbf{TODO:} required identities for $+$ and $*$ from Spivak or Halmos.
\end{itemize}
\end{plDefinition}

Usually the distinction between $\Set{V}$ and $\Space{V}$ 
is ignored, and we will say, for example, 
$\Vector{v} \in \Space{V}$.

\begin{plDefinition}{Linear dependence}{}
Suppose $\Space{V}$ is a linear space and
$\Vector{v}_0 \ldots  \Vector{v}_{n-1} \in \Space{V}$.
If there exists $a_0 \ldots  a_{n-1}$ such that
$\Vector{0} = \sum a_i \Vector{v}_i$ then the $\left\{ \Vector{v}_i \right\}$
are \textit{linearly dependent}.
\citeAuthorYearTitle[section~5]{Halmos:1958:Finite}

Otherwise they are \textit{linearly independent}.
\end{plDefinition}

\begin{plLemma}{The set of non-zero vectors
 $\Vector{v}_0 \ldots \Vector{v}_{n-1}$
is linearly dependent iff some $\Vector{v}_k, \; 1 \leq n-1$, 
is a linear combination of the preceding 
ones \citeAuthorYearTitle[Section 6]{Halmos:1958:Finite}.}{linearDependenceLemma}
\textsc{Proof:}
Assume  $\Vector{v}_0 \ldots \Vector{v}_{n-1}$ are linearly dependent.
Consider the smallest $k$ such that 
$\Vector{v}_0 \ldots \Vector{v}_{k}$ is linearly dependent.
By definition,
there exists non-zero $a_0 \ldots a_{k}$ such that
\begin{equation}
0 = \sum_0^k a_i \Vector{v}_i
\end{equation}
which implies that
\begin{equation}
\Vector{v}_k = \sum_0^{k-1} \frac{a_i}{- a_k} \Vector{v}_i
\end{equation}
\end{plLemma}

\begin{plTheorem}{The set of non-zero vectors
 $\Vector{v}_0 \ldots \Vector{v}_{n-1}$
is linearly dependent iff some $\Vector{v}_k, \; 1 \leq n-1$, 
is a linear combination of the preceding 
ones \citeAuthorYearTitle[Section 6]{Halmos:1958:Finite}.}{linearDependenceTheorem}
\textsc{Proof:}
Assume  $\Vector{v}_0 \ldots \Vector{v}_{n-1}$ are linearly dependent.
Consider the smallest $k$ such that 
$\Vector{v}_0 \ldots \Vector{v}_{k}$ is linearly dependent.
By definition,
there exists non-zero $a_0 \ldots a_{k}$ such that
\begin{equation}
0 = \sum_0^k a_i \Vector{v}_i
\end{equation}
which implies that
\begin{equation}
\Vector{v}_k = \sum_0^{k-1} \frac{a_i}{- a_k} \Vector{v}_i
\end{equation}
\end{plTheorem}

\begin{plCorollary}{The set of non-zero vectors
 $\Vector{v}_0 \ldots \Vector{v}_{n-1}$
is linearly dependent iff some $\Vector{v}_k, \; 1 \leq n-1$, 
is a linear combination of the preceding 
ones \citeAuthorYearTitle[Section 6]{Halmos:1958:Finite}.}{linearDependenceCorollary}
\textsc{Proof:}
Assume  $\Vector{v}_0 \ldots \Vector{v}_{n-1}$ are linearly dependent.
Consider the smallest $k$ such that 
$\Vector{v}_0 \ldots \Vector{v}_{k}$ is linearly dependent.
By definition,
there exists non-zero $a_0 \ldots a_{k}$ such that
\begin{equation}
0 = \sum_0^k a_i \Vector{v}_i
\end{equation}
which implies that
\begin{equation}
\Vector{v}_k = \sum_0^{k-1} \frac{a_i}{- a_k} \Vector{v}_i
\end{equation}
\end{plCorollary}

Examples:

\begin{plExample}{$\Space{K}^n$}{}
%{}
%\bigskip
Where $\Space{K}$ is any field.
%\leavevmode \vspace{-\baselineskip}
\begin{itemize}
  \item vectors:
  $\Set{V} = \Space{K}^n = \left\{ \Vector{x}
  = \left[ x_0 \ldots  x_{n-1} \right] \right\}$,
  tuples of $n$ elements $x_i \in \Space{K}$.
  \item scalars: $\Space{K}$
  \item scalar multiplication:
  $ a *_{\Space{K}^n} \left[ x_0 \ldots  x_{n-1} \right] =
  \left[ \ldots , a *_{\Space{K}} x_{i} , \ldots \right]$
  \item vector addition:
  $\Vector{x} +_{\Space{K}^n} \Vector{y}
  = \left[ \ldots , \left( x_i +_{\Space{K}} y_i \right) ,\ldots \right]$
\end{itemize}
\end{plExample}

\begin{plExample}{$\Space{R}^n$}{}
\vspace{\topsep}
\setlength{\parskip}{5pt}
\setlength{\parindent}{0pt}
See 
\citeAuthorYearTitle[chapter~1]{Spivak:1965:CalculusOnManifolds} 
and \citeAuthorYearTitle[chapter~1]{Halmos:1958:Finite}.
%\leavevmode \vspace{-\baselineskip}
\begin{itemize}
  \item vectors:
  $\Set{V} = \Space{R}^n = \left\{ \Vector{x}
  = \left[ x_0 \ldots  x_{n-1} \right] \right\}$,
  tuples of $n$ elements $x_i \in \Space{R}$.
  \item scalars: $\Space{R}$
  \item scalar multiplication:
  $ a*\left[ x_0 \ldots  x_{n-1} \right] =
  \left[ \ldots , \left( a*x_i \right) , \ldots \right]$
  \item vector addition:
  $\Vector{x} + \Vector{y}
  = \left[ \ldots , \left( x_i + y_i \right) , \ldots \right]$
\end{itemize}

\textbf{TODO: DANGER:} Apple-orange mistakes resulting from
using $\Space{R}^n$ in problems where coordinates don't mean the
same thing, so canonical inner product and $l2$ distance
aren't correct.

Homogeneous problems often don't have meaningful coordinates.

Can only approximate $\Space{R}^n$ with tuples of \texttt{double},
which is fundamentally different due to lack of associativity,
which leads to accumulation of rounding error.

\end{plExample}

\textbf{TODO:} Exact float arithmetic as an alternative?

\begin{plExample}{$\Space{Q}^n$}{}
%{}\bigskip
Like $\Space{R}^n$, only over rational rather than real numbers.
Has the advantage that it can be implemented accurately
using arbitrary precision fractions, though at considerable
space-time cost.

\textbf{TODO:} measure cost compared to \texttt{double}
approximation to $\Space{R}^n$
\end{plExample}


%\begin{plExample}{$ \Space{F} \left[ \Set{D}, \Space{V} \right] $ }{}
\begin{plExample}{The functions from any domain to some linear space.}{}
The functions from any domain to some linear space.
\textbf{TODO:} lisp notation for clarity below.
\begin{itemize}
  \item vectors: $\Set{F} = $ any function on $\Set{D}$
  that returns values in the linear space $\Space{V}$.
  \item scalars: $\Space{K}$, the same scalar field used by
  $\Space{V}=\left[ \Set{V}, \Space{K}, +, * \right]$.
  \item scalar multiplication:
  $ \left(a*\Vector{f}\right) : \Set{D} \rightarrow \Space{V}$
  is the function defined by
   $ \left(a*\Vector{f}\right) (x)
   = a*\left(\Vector{f}\left( x \right) \right) $
  \item vector addition:
  $\left( \Vector{f} + \Vector{g} \right) $
  is the function defined by
  $\left( \Vector{f} + \Vector{g} \right) \left( x \right) =
  \Vector{f} \left( x \right) + \Vector{g} \left( x\right)$
\end{itemize}
Canonical coordinates: 
$\Vector{f}\left( d \right) \; \forall d \in \Set{D}$
\end{plExample}

\begin{plExample}{{ $\Space{R}^n$ as a function space }}{}
%{}
%\bigskip
We can identify \citeAuthorYearTitle[sec.~24]{Halmos:1958:Finite} 
$\Space{R}^n$ and 
$ \Space{F} \left[ \left\{ 0, 1, \ldots, {n-1} \right\}, \Space{R} \right] $
by
$\Vector{x} \Leftrightarrow f_{\Vector{x}}$
where $f_{\Vector{x}} \left( i \right) = \Vector{x}_i$
for $\Vector{x} \in \Space{R}^n$ and 
$i \in \left\{ 0, 1, \ldots, {n-1} \right\}$
\end{plExample}

%-----------------------------------------------------------------
\begin{plDefinition}{Linear dependence}{}
Suppose $\Space{V}$ is a linear space and
$\Vector{v}_0 \ldots  \Vector{v}_{n-1} \in \Space{V}$.
If there exists $a_0 \ldots  a_{n-1}$ such that
$\Vector{0} = \sum a_i \Vector{v}_i$ then the $\left\{ \Vector{v}_i \right\}$
are \textit{linearly dependent}.
\citeAuthorYearTitle[section~5]{Halmos:1958:Finite}

Otherwise they are \textit{linearly independent}.
\end{plDefinition}

\begin{plTheorem}{The set of non-zero vectors
 $\Vector{v}_0 \ldots \Vector{v}_{n-1}$
is linearly dependent iff some $\Vector{v}_k, \; 1 \leq n-1$, 
is a linear combination of the preceding 
ones \citeAuthorYearTitle[Section 6]{Halmos:1958:Finite}.}{linearDependence}
\textsc{Proof:}
Assume  $\Vector{v}_0 \ldots \Vector{v}_{n-1}$ are linearly dependent.
Consider the smallest $k$ such that 
$\Vector{v}_0 \ldots \Vector{v}_{k}$ is linearly dependent.
By definition,
there exists non-zero $a_0 \ldots a_{k}$ such that
\begin{equation}
0 = \sum_0^k a_i \Vector{v}_i
\end{equation}
which implies that
\begin{equation}
\Vector{v}_k = \sum_0^{k-1} \frac{a_i}{- a_k} \Vector{v}_i
\end{equation}

\end{plTheorem}
%-----------------------------------------------------------------
\begin{plSection}{Bases}
\end{plSection}%{Bases}
%-----------------------------------------------------------------
\begin{plSection}{Dimension}
\end{plSection}%{Dimension}
%-----------------------------------------------------------------
\begin{plSection}{Subspaces}

A subset which is also a linear space with the
same scalars and operations.

\begin{plExample}{Canonical subspaces of a function space}{}
\bigskip
Let $\Set{D}_0 \subset \Set{D}_1$ be a proper subset.
Consider 
$\Space{F}_0 = \Space{F} \left[ \Set{D}_0, \Space{V} \right] $
and
$\Space{F}_1 = \Space{F} \left[ \Set{D}_1, \Space{V} \right] $.
\end{plExample}

\begin{plExample}{Canonical subspaces of $\Space{R}^n$}{}
\bigskip
Finite index set of integers, a real value for each.
Relationships between index sets define super/sub space relations
intersections.
\end{plExample}
\end{plSection}%{Subspaces}
%-----------------------------------------------------------------
\begin{plSection}{Normed linear spaces}
\end{plSection}%{Normed linear spaces}
%-----------------------------------------------------------------
\begin{plSection}{Inner product (linear) spaces}
Let $\Space{V}$ be an $n$-dimensional real inner product space.
Let $\Vector{v}, \Vector{w} \in \Space{V}$.

\begin{itemize}
\item The inner (dot) product on $\Reals^n$:
\begin{equation}
\Vector{v} \bullet \Vector{w} \; \equiv \; \sum_{i=0}^{n-1} v_i w_i
\end{equation}

\item The euclidean ($l_2$) norm:
\begin{equation}
\| \Vector{v} \|^2 \; \equiv \; \Vector{v} \bullet \Vector{v}
\end{equation}

\item $\theta(\Vector{v},\Vector{w})$ 
is the angle between $\Vector{v}$ and $\Vector{w}$
and is defined by:
\begin{eqnarray}
\Vector{v} \bullet \Vector{w} 
\; = \; \| \Vector{v} \| \| \Vector{w} \| 
\cos(\theta(\Vector{v},\Vector{w}))
\\
\theta(\Vector{v},\Vector{w})
\; \equiv \;
\cos^{-1} 
\left(
\frac{ \Vector{v} \bullet \Vector{w} }
{\| \Vector{v} \| \| \Vector{w} \| } 
\right)
\nonumber
\end{eqnarray}

\item The tensor (outer) product:

Let $\Vector{v}, \Vector{u} \in \Space{V}, \Vector{w} \in \Space{W}.$
$\Vector{w} \otimes \Vector{v}$ is a rank 1 linear map
from $\Space{V}$ to $\Space{W}$, defined by:
\begin{equation}
(\Vector{w} \otimes \Vector{v})(\Vector{u}) \; \equiv \; \Vector{w} (\Vector{v} \bullet \Vector{u})
\end{equation}

Note: this is an abuse of the usual definition of tensor product $\otimes$.
This operation, which takes a pair of vectors and returns a linear map,
is more conventionally referred to as the 'outer product',
and written $\Vector{w} \Vector{v}^{\dagger}$.
However, because I am working in spaces other than $\Reals^n$
(eg. $\Space{L}(\Space{V},\Space{W})$, the space of linear maps
between 2 vector spaces),
I want to avoid notations that suggest thinking in terms
of 'row' and 'column' vectors.

The following is a useful identity.
If $\Vector{t} \in \Space{T}$, $\Vector{u}, \Vector{v} \in \Space{V}$, and $\Vector{w} \in \Space{W}.$
then
\begin{equation}
\label{eq:tensor-dot}
(\Vector{t} \otimes \Vector{u}) (\Vector{v} \otimes \Vector{w})(\Vector{u}) = (\Vector{u} \bullet \Vector{v}) (\Vector{t} \otimes \Vector{w})
\end{equation}

\item Elementary orthogonal projection:
\begin{equation}
\Projection_{\Vector{w}} \Vector{v}
\; \equiv \;
\left( \frac{ \Vector{w} }{ \| \Vector{w} \| } \otimes \frac{ \Vector{w} }{ \| \Vector{w} \| } \right) \Vector{v}
\; = \;
\left( \frac{\Vector{w} }{\|\Vector{w}\|} \bullet \Vector{v} \right) \frac{\Vector{w}}{\|\Vector{w}\|}
\end{equation}

\item Orthogonal complement:
\begin{equation}
\perp_{\Vector{w}} \Vector{v}
\; \equiv \;
\Vector{v} \perp \Vector{w}
\; \equiv \;
\Vector{v} \; - \; \Projection_{\Vector{w}} \Vector{v}
\; = \;
\Vector{v} \; - \; \left( \frac{\Vector{w}}{\|\Vector{w}\|} \bullet \Vector{v} \right) \frac{\Vector{w}}{\|\Vector{w}\|}
\end{equation}

\end{itemize}
\end{plSection}%{Inner product (linear) spaces}
%-----------------------------------------------------------------
\end{plSection}%{Linear spaces}
%-----------------------------------------------------------------
\begin{plSection}{Affine spaces}
\label{sec:affine-spaces}
%-----------------------------------------------------------------
\begin{plSection}{Euclidean space}
\end{plSection}%{Euclidean space}
%-----------------------------------------------------------------
\end{plSection}%{Affine spaces}
%-----------------------------------------------------------------
\begin{plSection}{Projective spaces}
\label{sec:Projective-spaces}
%-----------------------------------------------------------------
\begin{plSection}{Oriented projective spaces}
\label{sec:Oriented-projective-spaces}
\citeAuthorYearTitle{Stolfi:1991:OrientedProjectiveGeometry}
\end{plSection}%{Oriented projective spaces}
%-----------------------------------------------------------------
\end{plSection}%{Projective spaces}
%-----------------------------------------------------------------
\begin{plSection}{Barycentric (convex) spaces and functions}
\end{plSection}%{Barycentric (convex) spaces and functions}
%-----------------------------------------------------------------
\begin{plSection}{Spherical spaces}
\end{plSection}%{Spherical spaces}
%-----------------------------------------------------------------
\begin{plSection}{Manifolds}
\label{sec:Manifolds}
\end{plSection}%{Manifolds}
%-----------------------------------------------------------------
\begin{plSection}{Functions between spaces}
\label{sec:Functions-between-spaces}

In general, the functions discussed here map between real inner product spaces:
$\Vector{f}:\Space{V} \mapsto \Space{W}$, where $\Space{V}$ is the
\textit{domain} and $\Space{W}$ is the \textit{codomain}.
The real inner product spaces are almost derived from some $\Reals^n$.

The \textit{range} of $\Vector{f}$, $\range(\Vector{f})$, is the set $\Vector{f}(\Space{V})$,
which may be a proper subset of its codomain $\Space{W}$.
The \textit{kernel} of $\Vector{f}$, $\kernel(\Vector{f})$, is the set
$\kernel(\Vector{f}) = \left\{ \Vector{v} \in \Space{V} : \Vector{f}(\Vector{v}) = \Vector{0} \right\}$.

When I want to distinguish between real- and vector-valued functions,
I may use 'function' for vector-valued functions and
'functional' for real-valued ones.

I use $\Space{U}$, $\Space{V}$, $\Space{W}$ for generic linear spaces,
$\Vector{u}$, $\Vector{v}$, $\Vector{w}$, etc., for elements of linear spaces,
usually called \textit{vectors}
and
$\Vector{f}$, $\Vector{g}$, $\Vector{h}$ for vector-valued functions.
I generally do not distinguish $\Reals$, the real numbers,
and $\Reals^1$, or any other 1-dimensional real linear space.
I sometimes use $f$, $g$, $h$ for extra clarity in the special
case of real-valued functions.

The domains of many interesting functions,
such as those that depend on vertex positions,
are direct sum of inner product spaces.
The \textit{direct sum} $\Space{V} \oplus \Space{W}$ is the inner product space
consisting of the ordered pairs $\left\{ (\Vector{v},\Vector{w}) : \Vector{v} \in \Space{V}, \Vector{w} \in \Space{W} \right\}$
inheriting the inner product space operations in the obvious way:
$(\Vector{v}_0,\Vector{w}_0) \bullet (\Vector{v}_1,\Vector{w}_1) = (\Vector{v}_0 \bullet \Vector{v}_1) + (\Vector{w}_0 \bullet \Vector{w}_1).$
I will usually write an element of $\oplus^n \Space{V}$ as
$(\Vector{v}_0,\ldots,\Vector{v}_{n-1})$
and use
$\Vector{f}(\Vector{v}_0,\Vector{v}_1,\ldots,\Vector{v}_{n-1})$
for a function that depends on $n$ vectors.

%-----------------------------------------------------------------
\begin{plSection}{Linear functions}
\label{sec:linear-functions}

A function $\Vector{L}(\Vector{v}):\Space{V} \mapsto \Space{W}$
is \textit{linear} iff
$\Vector{L}(a_0 \Vector{v}_0 + a_1 \Vector{v}_1) = a_0 \Vector{L}(\Vector{v}_0) + a_1 \Vector{L}(\Vector{v}_1)$.
I will often write $\Vector{L}\Vector{v} \equiv \Vector{L}(\Vector{v})$.

It is not hard to see that, for a linear function,
the range and kernel are linear subspaces of the codomain and
domain, respectively.
Thus any linear function between inner product spaces
divides its domain and codomain each into 2 orthogonal subspaces.
The domain is divided into $\Space{V} = \kernel(\Vector{L}) \oplus \kernel^{\perp}(\Vector{L})$,
and the codomain is divided into $\Space{W} = \range(\Vector{L}) \oplus \range^{\perp}(\Vector{L})$.

The most common representation for linear functions is the \textit{matrix:}
Let $\Vector{L}(\Vector{v}):\Space{V} \mapsto \Space{W}$ be linear,
$\left\{ \Vector{e}_0^{\Space{V}} \ldots  \Vector{e}_{m-1}^{\Space{V}} \right\}$ an orthonormal basis for $\Space{V}$,
and
$\left\{ \Vector{e}_0^{\Space{W}} \ldots \Vector{e}_{n-1}^{\Space{W}} \right\}$ an orthonormal  basis for $\Space{W}$
Then $\Vector{L}$ can be expressed as
\begin{equation}
\Vector{L}
 =
\sum_{i=0}^{m-1} \sum_{j=0}^{n-1} L_{ij} ( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} )
\end{equation}
$(L_{ij})$ is the matrix representation of $\Vector{L}$ with respect to
the two bases \citeAuthorYearTitle{Halmos:1958:Finite}.

It is important to note that there are many useful
representations for linear functions other than 
matrices \citeAuthorYearTitle{McDonald:1989:OOPSLA}.
Sometimes other representations are used for convenience,
or to enforce some constraint like symmetry.
In some cases, a non-matrix representation must be used,
because a particular linear transformation
cannot be accurately represented by a matrix of floating point numbers.

Examples:

\begin{itemize}

\item Column-wise:
$\Vector{L} = \sum_{j=0}^{n-1} ( \Vector{c}_j^{\Vector{L}} \otimes \Vector{e}_j^{\Space{V}} )$

$\Vector{c}_j^{\Vector{L}} \in \Space{W}$ are the 'columns' of $\Vector{L}$.
$\linearspan\left\{ \Vector{c}_0^{\Vector{L}} \ldots \Vector{c}_{n-1}^{\Vector{L}} \right\} = \range(\Vector{L})$
(see \cref{sec:spans-and-projections}).

\item Row-wise:
$\Vector{L} = \sum_{i=0}^{m-1} ( \Vector{e}_i^{\Space{W}} \otimes  \Vector{r}_i^{\Vector{L}} )$

$\Vector{r}_i^{\Vector{L}} \in \Space{V}$ are the 'rows' of $\Vector{L}$.
$\linearspan\left\{ \Vector{r}_0^{\Vector{L}} \ldots \Vector{r}_{m-1}^{\Vector{L}} \right\} =  \kernel(\Vector{L})^{\perp}$
(see \cref{sec:spans-and-projections}).

\item Householder:
$\Vector{h}_{\Vector{v}} = \Identity_{\Space{V}} - \frac{2}{\| \Vector{v} \|^2} (\Vector{v} \otimes \Vector{v})$

Householder maps are usually chosen to zero the elements of
a vector, or a row or column of a matrix, for a contiguous range of
indices, say, $[i_0,\ldots,i_n)$.

\end {itemize}

\end{plSection}%{Linear functions}
%-----------------------------------------------------------------
\begin{plSection}{Affine functions}
\label{sec:affine-functions}

A function $\Point{A}(\Vector{v}):\Space{V} \mapsto \Space{W}$
is \textit{affine} if distributes over affine combinations:
$\Point{A}(\sum_{i=0}^{n-1} a_i \Vector{v}_i) = \sum_{i=0}^{n-1} a_i \Point{A}(\Vector{v}_i) $
for all $\left\{a_i\right\}$ such that $1 = \sum_{i=0}^{n-1} a_i$.
(Note that I am describing affine functions on vector (linear) spaces,
rather than the slightly more general notion of affine functions on affine spaces.)
Any linear function between linear spaces is automatically affine.
The other major class of affine functions on linear spaces are the translations.
A \textit{translation,} $\Point{T}_{\Vector{t}}$, $\Space{V} \mapsto \Space{V}$,
simply adds a vector ($\Vector{t}$) to its argument:
$\Point{T}_{\Vector{t}} \Vector{v} = \Vector{v} + \Vector{t}$.
It's not hard to see that any affine function between two linear spaces
can be represented as the sum of a linear function and a translation.
A typical representation for a general affine function $\Point{A} : \Space{V} \mapsto \Space{W}$
is as a pair $(\Vector{L},\Vector{t})$ where $\Vector{L} : \Space{V} \mapsto \Space{W}$ is linear,
$\Vector{t} \in \Space{W}$, and $\Point{A}(\Vector{v}) = \Vector{L}(\Vector{v}) + \Vector{t}$.

\end{plSection}%{Affine functions}
%-----------------------------------------------------------------
\begin{plSection}{Spans and projections}
\label{sec:spans-and-projections}

Let $\Space{V}$ be an $n$-dimensional inner product space.

The \textit{linear span} of a set of $m$ vectors in $\Space{V}$
is the set of linear combinations of those vectors:
\begin{equation}
\linearspan\left\{ \Vector{v}_0 \ldots \Vector{v}_{m-1} \right\} = \left\{\Vector{v} \in \Space{V} : \Vector{v} = \sum_{i=0}^{m-1} a_i \Vector{v}_i\right\}
\end{equation}
$\linearspan\left\{ \Vector{v}_0 \ldots \Vector{v}_{m-1} \right\}$ is a linear subspace of $\Space{V}$.

The \textit{projection} $\Projection_{\Set{S}} \Vector{v}$ of a vector $\Vector{v} \in \Space{V}$
onto an arbitrary subset $\Set{S} \subset \Space{V}$
is the closest point in $\Set{S}$ to $\Vector{v}$.
Projection onto a linear subspace is a linear function and
can be computed by summing
elementary orthogonal projections onto an orthonormal basis for the subspace.

An orthonormal basis for $\linearspan\left\{ \Vector{v}_0 \ldots \Vector{v}_{m-1} \right\}$
(and $\linearspan\left\{ \Vector{v}_0 \ldots \Vector{v}_{m-1} \right\}^\perp$)
can be computed using the QR decomposition
of the function $\Vector{V} = \sum_{i=0}^{m-1} \Vector{v}_i \otimes \Vector{e}_i$,
(the $n \times m$ matrix whose columns are the $\Vector{v}_i$)
(see \citeAuthorYearTitle[sec.~5.2 ]{GolubVanLoan:2012}).

The \textit{affine span} of a set of $m+1$ vectors in $\Space{V}$
is the set of affine combinations of those vectors:
\begin{equation}
\affinespan\left\{ \Point{p}_0 \ldots \Point{p}_{m} \right\} = \left\{\Vector{v} \in \Space{V} : \Vector{v} = \sum_{i=0}^{m} b_i \Point{p}_i;
1 = \sum_{i=0}^{m} b_i \right\}.
\end{equation}
$\affinespan\left\{ \Point{p}_0 \ldots \Point{p}_{m} \right\}$ is an affine subspace of $\Space{V}$.
$\Vector{b} = ( b_0 \ldots b_m )$ are \textit{barycentric coordinates}
for $\Vector{v}$ with respect to $\left\{ \Point{p}_0 \ldots \Point{p}_{m} \right\}$.
The barycentric coordinates are unique if $\left\{ \Point{p}_0 \ldots \Point{p}_{m} \right\}$
are affinely independent.

Any affine subspace, $\Space{A}$, of a linear space, $\Space{V}$ can be represented as
as a translation of a linear subspace of $\Space{V}$:
$\Space{A} = \Space{T}(\Space{A}) + \Vector{t}$,
$\Space{T}(\Space{A})$ is the set of differences of elements of $\Space{A}$,
a linear subspace of $\Space{V}$.
If $\Vector{t}$ is any element of $\Space{A}$.
then projection onto $\Space{A}$
can be computed as a translation of an orthogonal projection onto $\Space{T}(\Space{A})$:
$\Projection_{\Space{A}} (\Point{p}) = \Vector{t} + \Projection_{\Space{T}(\Space{A})} (\Point{p} - \Vector{t})$.
Typically, we pick $\Vector{t}$ to be the smallest element of $\Space{A}$.
Projection onto an affine space is clearly an affine function.

We can represent the affine span of a set of $m+1$ vectors
as a translation of a linear span:
\begin{equation}
\affinespan\left\{ \Point{p}_0 \ldots \Point{p}_{m} \right\} = \Point{p}_m + \linearspan\left\{\Vector{v}_0 \ldots \Vector{v}_{m-1}\right\}
\end{equation}
where $\Vector{v}_i = \Point{p}_i - \Point{p}_m$,
which allows us to compute the projection onto
$\affinespan\left\{ \Point{p}_0 \ldots \Point{p}_{m} \right\}$
again using the QR decomposition
of $\Vector{V} = \sum_{i=0}^{m-1} \Vector{v}_i \otimes \Vector{e}_i$.

\end{plSection}%{Spans and projections}
%-----------------------------------------------------------------
\begin{plSection}{Linear inverses and pseudo-inverses}
\label{sec:Linear-inverses-and-pseudo-inverses}

A convenient definition for the \textit{true inverse}
of a function $\Vector{f}(\Vector{v}):\Space{V} \mapsto \Space{W}$ is
$\Vector{f}^{-1}(\Vector{w}) = \left\{ \Vector{v} : \Vector{f}(\Vector{v}) = \Vector{w} \right\}$.
The usual definition of inverse treats $\Vector{f}^{-1}$
as a function from $\Space{W} \mapsto \Space{V}$,
which is undefined where the value of the true
inverse is not a set containing a single point.

For functions between inner product spaces,
the \textit{pseudo-inverse}, $f^{-}$, is a function $\Space{W} \mapsto \Space{V}$
defined everywhere on $\Space{W}$.
Let $\hat{\Vector{w}}$ be an element of $\Space{W}$ closest to $\Vector{w}$
such that $\Vector{f}^{-1}(\Vector{w})$ is not empty.
Let $\hat{\Vector{v}}$ be a minimum norm element of $\Vector{f}^{-1}(\hat{\Vector{w}})$.
Then $\Vector{f}^{-}(\Vector{w}) = \hat{\Vector{v}}$.

If $\Vector{L}$ is linear, then it's not hard to see that
$\hat{\Vector{w}} = \pi_{\range(\Vector{L})} \Vector{w}$, the projection of $\Vector{w}$
on the range of $\Vector{L}$
and
$\hat{\Vector{v}}$ is the unique element of $\kernel^{\perp}(\Vector{L})$
such that $\Vector{L}(\hat{\Vector{v}}) = \hat{\Vector{w}}$.

The pseudo-inverse of a linear function can be characterized
by the four Moore-Penrose 
conditions \citeAuthorYearTitle[sec.~5.5.2]{GolubVanLoan:2012}:
\begin{enumerate}
\item $\Vector{L} \Vector{L}^{-} \Vector{L} = \Vector{L}$
\item $\Vector{L}^{-} \Vector{L} \Vector{L}^{-} = \Vector{L}^{-}$
\item $\left( \Vector{L} \Vector{L}^{-} \right)^{\dagger} = \Vector{L} \Vector{L}^{-}$
\item $\left( \Vector{L}^{-} \Vector{L} \right)^{\dagger} = \Vector{L}^{-} \Vector{L}$
\end{enumerate}

When the 'columns' of $\Vector{L}$, $\Vector{r}_j^{\Vector{L}}$
($\Vector{L} = \sum_{j=0}^{n-1} ( \Vector{L}_j^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} )$)
are linearly independent,
then a useful identity is:
\begin{equation}
\label{eq:full-rank-pseudo-inverse}
\Vector{L}^{-} = \left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1} \Vector{L}^{\dagger}
\end{equation}

The pseudoinverse can be computed
using standard matrix decompositions such as
the QR and SVD \citeAuthorYearTitle{GolubVanLoan:2012}.
The pseudoinverse is an example of a linear transformation
which should {\em not} be represented by a matrix
\citeAuthorYearTitle{McDonald:1989:OOPSLA}.

If $\Point{A}$ is affine,
let $\Point{A} = \Vector{L} + \Vector{t}$,
where $\Vector{L}$ is linear,
and $\Vector{t}$ is an element of $\range(\Point{A})$.
Then $\Point{A}^{-}(\Vector{w}) = \Vector{L}^{-}( \Vector{w} - \Vector{t} )$.
\end{plSection}%{Linear inverses and pseudo-inverses}
%-----------------------------------------------------------------
\end{plSection}%{Functions between spaces}
%-----------------------------------------------------------------
\end{plSection}%{Spaces}
%-----------------------------------------------------------------
