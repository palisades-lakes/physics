\begin{plSection}{Averaging}
\label{sec:Averaging}

A {\it mesh catalog} is a set 
$\left\{ \Set{M}_0 \ldots \Set{M}_{n-1} \right\}$
of $n$ meshes which are all embeddings
of the same simplicial complex
(that is, they have the same vertices, edges, and faces,
but the vertex positions may be different).

%-----------------------------------------------------------------
\begin{plSection}{Linear weights}
\label{sec:Linear-weights}

We can compute a {\it weighted average mesh} 
by averaging the vertex positions:
\begin{equation}
\Vector{p}_i(\Vector{w}) = \sum_{j=0}^{n-1} w_j \Vector{p}_{ij}
\end{equation}
where $\Vector{p}_{i}; (i=0 \ldots m-1)$ is the position of the $i$th vertex of the average mesh,
and $\Vector{p}_{ij}$ is the position of the $i$th vertex of the $j$th catalog element.

In {\it catalog fitting} we fit a registered weighted average mesh,
to a set of data, $\left\{ \Vector{d}_k \in \Reals^3; k=0 \ldots p-1 \right\}$
by minimizing
\begin{equation}
f(\Set{M}(\Vector{T},\Vector{w})) = \sum_{k=0}^{p-1} \| \Vector{d}_k - \Projection_{\Set{M}(\Vector{T},\Vector{w})} (\Vector{d}_k) \|^2 ,
\end{equation}
over a family of registration transforms $\left\{\Vector{T}\right\}$
and vectors of weights $\Vector{w} \in \Reals^n$.
The vertex positions $\Vector{p}(\Vector{T},\Vector{w}) = (\Vector{p}_0(\Vector{T},\Vector{w}) \ldots  \Vector{p}_{n-1}(\Vector{T},\Vector{w}))$
of the registered average mesh, $\Set{M}(\Vector{T},\Vector{w})$, are given by:
\begin{equation}
\Vector{p}_i(\Vector{T},\Vector{w}) = \Vector{T} ( \sum_{j=0}^{n-1} w_j \Vector{p}_{ij} )
\end{equation}

The partial derivative with respect to $\Vector{T}$
is:
\begin{eqnarray}
\Derivative[\Vector{T}]
{f(\Set{M}(\Vector{T},\Vector{w}))}[\Vector{T}^0,\Vector{w}^0]
& = &
\Derivative[\Vector{p}]
{f(\Set{M}))}[\Set{M}(\Vector{T}^0,\Vector{w}^0)]
\circ
\Derivative[\Vector{T}]
{\Vector{p}(\Vector{T},\Vector{w})}[\Vector{T}^0,\Vector{w}^0]
\\
& = &
\Derivative[\Vector{p}]
{f(\Set{M}))}[\Set{M}(\Vector{T}^0,\Vector{w}^0)]
\circ
\Derivative[\Vector{T}]
{\Vector{T}\Vector{p}(\Vector{w}^0)}[\Vector{T}^0]
\nonumber
\end{eqnarray}

$\Derivative[\Vector{p}]{f(\Set{M}))}[\Set{M}(\Vector{T}^0,\Vector{w}^0)]$ is the derivative of $f$ with respect to
the vertex positions of of the registered average mesh.
It's value for data fitting distance functions
is given in \cref{sec:data-fitting}.

$\Derivative[\Vector{T}]{\Vector{T}\Vector{p}}$ 
is given in \cref{sec:Transforms}
for various families of transforms $\left\{\Vector{T}\right\}$: affine, euclidean, and rigid.

Using the chain rule, 
the partial derivative with respect to $\Vector{w}$ is:
\begin{eqnarray}
\Derivative[\Vector{w}]
{f(\Set{M}(\Vector{T},\Vector{w}))}
[\Vector{T}^0,\Vector{w}^0]
& = &
\Derivative[\Vector{w}]
{f(\Vector{T}(\Set{M}(\Vector{w})))}
[\Vector{T}^0,\Vector{w}^0]
\\
& = &
\Derivative[\Vector{p}]
{f(\Set{M}))}
[\Set{M}(\Vector{T}^0,\Vector{w}^0)]
\circ
\Derivative[\Vector{T}
]{\Vector{T}\Vector{p}(\Vector{w}^0)}
[\Vector{T}^0]
\circ
\Derivative[\Vector{w}]{\Vector{p}(\Vector{w})}[\Vector{w}^0]
\nonumber
\end{eqnarray}

$\Derivative[\Vector{p}]{f}$ and 
$\Derivative[\Vector{T}]{\Vector{T}\Vector{p}}$ 
are already known
so we need only determine 
$\Derivative[\Vector{w}]{\Vector{p}(\Vector{w})}$.
Using reasoning similar
to \cref{eq:total-registration-transform-derivative},
we have 
$\Vector{p}(\Vector{w}) = 
\bigoplus_{j=0}^{m-1} \Vector{p}_i(\Vector{w})$,
and
$\Derivative[\Vector{w}]{\Vector{p}(\Vector{w})}
=
\Derivative[\Vector{w}]
{\bigoplus_{j=0}^{m-1} \Vector{p}_i(\Vector{w})}
=
\bigoplus_{j=0}^{m-1} 
\Derivative[\Vector{w}]{\Vector{p}_i(\Vector{w})}$,
so we can restrict ourselves to
$\Derivative[\Vector{w}]{\Vector{p}(\Vector{w})}$, where 
$\Vector{p}(\Vector{w}) = \sum_{i=0}^{n-1} w_i \Vector{p}_i$,
and $\Vector{p}, \Vector{p}_i \in \Reals^3$.

Note that $\Vector{p}(\Vector{w})$ is a linear transform 
from $\Reals^n \mapsto \Reals^3$,
which can be expressed as 
$\Vector{P} 
= \sum_{i=0}^{n-1} \Vector{p}_i \otimes \Vector{e}_i$,
where $\Vector{e}_i$ are the canonical basis vectors 
of $\Reals^n$.
($\Vector{P}$ can be written as a matrix 
whose $i$th column is the vector $\Vector{p}_i$.)
Thus it follows that
\begin{equation}
\Derivative[\Vector{w}]{\Vector{p}(\Vector{w})} 
= 
\Derivative[\Vector{w}]{\Vector{P}\Vector{w}} 
= \Vector{P}
\end{equation}

\end{plSection}%{Linear weights}
%-----------------------------------------------------------------
\begin{plSection}{Convex weights}
\label{sec:Convex-weights}

It may be desirable to restrict the weights to be convex,
that is, $0 \leq w_i \leq 1; \sum w_i = 1$.
To use unconstrained optimization methods with convex weights,
we re-parametrize:
\begin{equation}
u_i(\Vector{w}) = \frac{w_i^2}{\|\Vector{w}\|^2}
\end{equation}
where, as usual, $\| \Vector{w} \|^2 = \sum_{j=0}^{n-1} w_j^2$.

Then we need to compute
\begin{eqnarray}
\Derivative[\Vector{w}]{\Vector{p}(\Vector{u}(\Vector{w}))}[\Vector{w}^0]
& = &
\Derivative[\Vector{u}]{\Vector{p}(\Vector{u})}[\Vector{u}(\Vector{w}^0)]
\circ
\Derivative[\Vector{w}]{\Vector{u}(\Vector{w})}[\Vector{w}^0]
\\
& = &
\Vector{P}
\circ
\Derivative[\Vector{w}]{\Vector{u}(\Vector{w})}[\Vector{w}^0]
\nonumber
\end{eqnarray}

The partial derivatives are
\begin{eqnarray}
\Derivative[w_j]{u_i(\Vector{w}))}
& = &
\Derivative[w_j]{\frac{w_i^2}{\|\Vector{w}\|^2}}
\\
& = &
\frac{2 w_j}{\|\Vector{w}\|^4}
\left( \delta_{ij} \| \Vector{w} \|^2 - w_i^2 \right)
\nonumber
\end{eqnarray}

\end{plSection}%{Convex weights}
%-----------------------------------------------------------------
\end{plSection}%{Averaging}
