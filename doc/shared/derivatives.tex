%-----------------------------------------------------------------
\begin{plSection}{Derivatives}
\label{sec:Derivatives}

One way to view the derivative of a 
function (see \citeAuthorTitle{wiki:FrechetDerivative})
$\Vector{f} : \Space{V} \mapsto \Space{W}$,
at a point $\Vector{v}$,
is as the linear function $\Vector{L}:\Space{V} \mapsto \Space{W}$,
that best approximates the local 'slope' of $\Vector{f}$ at $\Vector{v}$.
(In the following, $\Vector{v}$, $\Vector{u}$, and $\Vector{t}$ are elements of $\Space{V}$.)
To be a little more precise, we want
\begin{equation}
\lim_{\|\delta\| \mapsto 0}
\frac{
\|
\Vector{f}(\Vector{v}+\delta)
\,-\,
 (\Vector{f}(\Vector{v}) + \Vector{L}(\delta))
\|}
{\|\delta  \| }
 = 0
\end{equation}

% \\De\left\{(.+)\right\}\left\{(.+)\right\}\left\{(.+)\right\}
% \\Derivative\[$1\]\left\{$2\right\}\[$3\]
\begin{plNote}{}{}
For a linear function $\Vector{L}$,
the derivative is constant over the domain
and the value is $\Vector{L}$ itself.
\end{plNote}

\textbf{TODO:} G\^{a}teaux derivative: collection of all
directional derivatives.

\textbf{TODO:} Generalized functions/distributions.

\textbf{TODO:}
relationship to currying and standard notation ambiguity vs
lisp notation.

For a concise and correct discussion, 
see \citeAuthorYearTitle[chapter 2]{Spivak:1965:CalculusOnManifolds}.

\textbf{TODO:} better notation for derivatives, especially 
'evaluated at' and partial.

\begin{itemize}

\item $\Derivative{\Vector{f}}$

In its most general form,
I denote the derivative of $\Vector{f}$ by $\Derivative{\Vector{f}}$.
Note that this is linear-function-valued function 
of the domain of $\Vector{f}$.

\item $\Derivative{\Vector{f}}[\Vector{u}]$

I denote the derivative of $\Vector{f}$ at $\Vector{u}$ by 
$\Derivative{\Vector{f}}[\Vector{u}]$.
$\Derivative{\Vector{f}}[\Vector{u}]$ 
is a specific linear transformation from
the domain of $\Vector{f}$ to the codomain of $\Vector{f}$.

\item $\Derivative{\Vector{f}}[\Vector{u}][\Vector{t}]$

The derivative is most often represented by the \textit{Jacobian},
the $m \times n$ matrix of partial derivatives
with respect to some bases for $\Space{V}$ and $\Space{W}$.
However, it's often easier to express the derivative clearly if we
explicitly include the argument of the linear transformation.
In this case, I write 
$\Derivative{\Vector{f}}[\Vector{u}][\Vector{t}]$
for the derivative of $\Vector{f}$ at the point $\Vector{u}$
applied to the vector $\Vector{t}$.

\item $\Derivative[\Vector{v}_i]{\Vector{f}}
[(\Vector{u}_0 \ldots \Vector{u}_{n-1})][\Vector{t}_i]$

For functions on direct sum spaces,
$\Vector{f}(\Vector{v}_0,\Vector{v}_1, \ldots, \Vector{v}_{n-1})$, 
$\Vector{v}_i \in \Space{V}_i$,
it's often easier to consider the derivative
with respect to one argument at a time.
I write 
$\Derivative[\Vector{v}_i]{\Vector{f}}
[(\Vector{u}_0 \ldots \Vector{u}_{n-1})]
[\Vector{t}_0 \ldots \Vector{t}_{n-1}]$
for the derivative of $\Vector{f}$ with respect to $\Vector{v}_i$,
at the point 
$(\Vector{u}_0 \ldots \Vector{u}_{n-1}) \in 
\oplus_{i=0}^{n-1} \Space{V}_i$,
applied to the vector $\Vector{t}_i \in \Space{V}_i$.

\item $\Partial{j}{\Vector{f}} = \Partial{v_j}{\Vector{f}}$

The traditional partial derivative of $\Vector{f}$ is with respect to
a single coordinate $v_j$ of the domain.
More formally, this is the directional derivative of $\Vector{f}$
in the direction of the $j$-th canonical basis vector 
$\Vector{e}_j^{\Space{V}}$.

$\Partial{j}{\Vector{f}}$ is a function from the domain 
of $\Vector{f}$ to the co-domain of $\Vector{f}$.
$\Partial{j}{\Vector{f}}[\Vector{u}]$ 
is the value of that function 
at $\Vector{u}$.
The partial derivative is related to the derivative by
\begin{eqnarray}
\label{eq:partial-full-dervatives}
\Derivative{\Vector{f}}[\Vector{u}]
& = &
\sum_{j=0}^{m-1} \Partial{j}{\Vector{f}}[\Vector{u}] 
\otimes \Vector{e}_j^{\Space{V}}
\\
\Partial{j}{\Vector{f}}[\Vector{u}]
& = &
\Derivative{\Vector{f}}[\Vector{u}] \Vector{e}_j^{\Space{V}}
\nonumber
\end{eqnarray}

\item $\Partial{j}{\Vector{f}_i}$

The Jacobian partial derivatives are the derivatives of
a particular coordinate of $\Vector{f}$, $\Vector{f}_i$, 
with respect to
a single coordinate $v_j$ of the domain.
$\Partial{j}{\Vector{f}_i}$ is a real-valued function 
on the domain of $\Vector{f}$.
$\Partial{j}{\Vector{f}_i}[\Vector{u}]$ 
is the value of that function at $\Vector{u}$.
The Jacobian partial derivatives form 
the 'matrix' representation of the derivative:
\begin{equation}
\Derivative{\Vector{f}}[\Vector{u}] =
\sum_{i=0}^{m-1}
\sum_{j=0}^{n-1}
\Partial{j}{\Vector{f}_i}[\Vector{u}]
\left( 
\Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} 
\right)
\end{equation}

\end{itemize}

In minimizing a real-valued function, 
$f(\Vector{v})$, $\Vector{v} \in \Space{V}$,
we frequently need to know 
both the direction of maximum increase of $f$
the rate of increase, or slope, of $f$ in that direction.

$\Gradient{f}$ is the \textit{gradient} of $f$.
The gradient has a close relationship to the derivative, $\Derivative{f}$,
and the two are often confused.
Recall that the derivative is a linear transformation
from the domain of $f$ to its codomain.
In the case of real-valued functions,
this means the derivative is a linear function on $\Space{V}$,
an element of the dual space of $\Space{V}$, a 'row' vector.
It's easy to see that the gradient is simply the dual (the 'transpose')
of the derivative, $\Gradient{f} = (\Derivative{f})^{\dagger}$
(see Spivak \cite[p.~96, ex.~4-18]{Spivak:1965:CalculusOnManifolds}).

$\Gradient{f}$ maps $\Space{V} \mapsto \Space{V}$.
$\Gradient{f}[\Vector{u}] \in \Space{V}$ 
is the gradient of $f$ at $\Vector{u} \in \Space{V}$;
it points in the direction of most rapid increase of
$f$ and its magnitude $\| \Gradient{f}[\Vector{u}] \|$ is the
slope of $f$ in that direction.

Notation for the various versions of the gradient
follows that for derivatives:
$\Gradient[\Vector{v}_i]{f}[\Vector{u}]$ 
is the partial gradient of $f$ with respect to $\Vector{v}_i$ at
$\Vector{u} = \left( \Vector{u}_0 \ldots \Vector{u}_{n-1} \right) 
\in \Space{V} = \oplus_{i=0}^{n-1} \Space{V}_i$
$\Gradient[\Vector{v}_i]{f}[\Vector{u}]$ 
is an element of $\Space{V}_i$.

$(\Gradient{f}[\Vector{u}]) \bullet  \Vector{t}$
and
$(\Gradient[\Vector{v}_i]{f}[\Vector{u}]) 
\bullet \Vector{t}_i$
are the analogs to expressing the derivative 
as a linear transformation with an explicit argument.
$(\Gradient{f}[\Vector{u}]) \bullet  \Vector{t}$ is a real number.
If we take $t$ to be the canonical basis for $\Space{V}$
we get an expression for $\Gradient{f}$ 
in terms of the partial derivatives of $f$:
\begin{equation}
\label{eq:gradient-from-partials}
\Gradient{f}[\Vector{u}] 
= \sum_{j=0}^{m-1} 
\left( 
\Partial{j}{f}[\Vector{u}] 
\right) \Vector{e}_j^{\Space{V}}
\end{equation}

$\Gradient{\Vector{f}_i}$ is the gradient 
of a particular (real-valued) coordinate
of a vector-valued function. 
It is related to the derivative $\Derivative{\Vector{f}}$
in a way similar to the relationship 
between $\Derivative{\Vector{f}}$ 
and its partials $\Partial{j}{\Vector{f}}$.
\begin{equation}
\Derivative{\Vector{f}}[\Vector{u}]
= \sum_{i=0}^{n-1}  
\Vector{e}_i^{\Space{W}} 
\otimes \Gradient{\Vector{f}_i}[\Vector{u}]
\end{equation}

The most general identity used in computing derivatives 
is the \textit{chain rule.}
Suppose
$\Vector{f}:\Space{U} \mapsto \Space{V}$,
$\Vector{g}:\Space{V} \mapsto \Space{W}$,
and
$\Vector{h} = \Vector{g} \circ \Vector{f} : \Space{U} \mapsto \Space{W}$
Then
\begin{equation}
\label{eq:chain-rule}
\Derivative{\Vector{h}}[\Vector{u}]
=  \Derivative{(\Vector{g} \circ \Vector{f})}[\Vector{v}]
=  \Derivative{\Vector{g}}[\Vector{f}(\Vector{v})]
  \circ  \Derivative{\Vector{f}}[\Vector{v}].
\end{equation}

It is sometimes useful to express this 
in terms of the partial derivatives:
\begin{equation}
\label{eq:chain-rule_partials}
\Derivative{\Vector{h}}[\Vector{u}] 
=  \sum_{i=0}^{n-1} 
\Partial{i}{\Vector{g}}[\Vector{f}(\Vector{u})] 
\otimes  
\Gradient{\Vector{f}_i}[\Vector{u}].
\end{equation}

See 
\citeAuthorYearTitle[Theorem~2-2]{Spivak:1965:CalculusOnManifolds}.

%-----------------------------------------------------------------
\begin{plSection}{Vector-valued functions}
\label{sec:Derivatives-of-Vector-valued-functions}

%-----------------------------------------------------------------
\begin{plSection}{Implicit functions}
\label{sec:Derivatives-of-implicit-functions}

Suppose 
$\Vector{f} : \Space{X} \times \Space{Y} \rightarrow \Space{Y}$,
for topological linear spaces $\Space{X}$ and $\Space{Y}$.

WLOG, consider the level set 
$\SetSpec{\left[ \Vector{x} , \Vector{y} \right]}
{\Vector{0} = \Vector{f}\left( \left[ \Vector{x} , \Vector{y} \right] \right)}$
(other level sets are equivalent to modifying $\Vector{f}$
by adding or subtracting 
an element of $\Space{Y}$).

All level sets are relations on $\Space{X} \times \Space{Y}$.
Under conditions on $\Derivative{\Vector{f}}$
(Spivak \cite[Theorem~2-12]{Spivak:1965:CalculusOnManifolds}),
there is a subset of the relation which is a 
function.

\textbf{TODO:} Implicit functions without differentiability
conditions, eg, when the level set isn't a smooth manifold,
maybe only piecewise smooth?

Specifically, 
there exists an open subset $\Set{D}  \subset \Space{X}$
and a differentiable function $\Vector{g} : \Set{D} \rightarrow \Space{Y}$
such that 
\begin{equation}\label{eq:Implicit-function}
\Vector{0} = \Vector{f}\left( \left[ \Vector{x} , \Vector{g} \left( \Vector{x} \right) \right] \right)
\end{equation}
for all 
$\Vector{x} \in  \Set{D}$.

\textbf{TODO:} Not really a very useful theorem --- 
how do you find $\Vector{g}$ and $\Set{D}$?
When is $\Set{D}$ big enough?

\textbf{TODO:} Can we extend $\Vector{g}$ to all of $\Space{X}$?

Note that 
\begin{equation}
\begin{aligned}
\Vector{h}\left(\Vector{x}\right) 
& = 
\Vector{f}\left( \left[ \Vector{x} , \Vector{g} \left( \Vector{x} \right) \right] \right)
\\
& =
\left( \Vector{f} \circ \left( \Vector{I}_{\Space{X}} \times \Vector{g} \right) \right) 
\left( \Vector{x} \right)
\end{aligned} 
\end{equation}

\Cref{eq:Implicit-function}, 
the chain rule (\cref{eq:chain-rule}),
and the linearity of $\Vector{I}_{\Space{X}}$
imply
\begin{equation}\label{eq:Implicit-derivative}
\begin{aligned}
\Vector{0}
 & = \Derivative{\Vector{h}}[\Vector{x}_0]
\\
& = 
\Derivative{\Vector{f}}
[\left[ 
\Vector{x}_0 , \Vector{g}\left( \Vector{x}_0 \right) 
\right]]
\circ \left( 
\Vector{I}_{\Space{X}} 
\times 
\Derivative{\Vector{g}}[\Vector{x}_0]
\right)
\\
& = 
\left(
\Derivative[\Vector{x}]
{\Vector{f}}
[\left[ 
\Vector{x}_0 , \Vector{g} \left( \Vector{x}_0 \right)
\right]]
\circ 
\Vector{I}_{\Space{X}}
\right)
% + 
% \left(
% \Derivative[\Vector{y}]{\Vector{f}}[\left[ \Vector{x}_0 , \Vector{g}\left( \Vector{x}_0 \right) \right]]
% \circ 
% \Derivative[\Vector{x}]{\Vector{g}}[#3]
% \right)
% \\
% & = 
% \Derivative[\Vector{x}]{\Vector{f}}[\left[ \Vector{x}_0 , \Vector{g}\left( \Vector{x}_0 \right) \right]]
% + 
% \left(
% \Derivative[\Vector{y}]{\Vector{f}}[\left[ \Vector{x}_0 , \Vector{g}\left( \Vector{x}_0 \right) \right]]
% \circ 
% \Derivative[\Vector{x}]{\Vector{g}}[\Vector{x}_0]
% \right)
\end{aligned}
\end{equation}

which implies

\begin{equation}
\Derivative[\Vector{x}]{\Vector{g}}[\Vector{x}_0]
=
\left(
\Derivative
[\Vector{y}]
{\Vector{f}}
[\left[ \Vector{x}_0 , \Vector{g}\left( \Vector{x}_0 \right) \right]]
\right)^{-1}
\circ 
\Derivative[\Vector{x}]
{\Vector{f}}
[\left[ 
\Vector{x}_0 , \Vector{g}\left( \Vector{x}_0 \right) 
\right]]
\end{equation}

When $\Space{Y}=\Space{R}$, this reduces to:
\begin{equation}
\Derivative[\Vector{x}]{\Vector{g}}[\Vector{x}_0]
=
\dfrac{
\Derivative[\Vector{x}]
{\Vector{f}}
[\left[
 \Vector{x}_0 , \Vector{g}\left( \Vector{x}_0 \right) 
 \right]]
}{
\Partial{\Vector{y}}
{\Vector{f}}
[\left[ 
\Vector{x}_0 , \Vector{g}\left( \Vector{x}_0 \right)
 \right]]
}
\end{equation}

\begin{plExample}{Circle}{}
$x^2 + y^2 - d = 0$
\end{plExample}

\end{plSection}%{Implicit functions}
%-----------------------------------------------------------------
\begin{plSection}{Multilinear functions}
\label{sec:Derivatives-of-multilinear-functions}

A function
 $\Vector{f}(\Vector{v}_0 \ldots \Vector{v}_k):\Space{V}_0 \oplus \ldots \oplus \Space{V}_k \mapsto \Space{W}$
is \textit{multilinear} if
\begin{equation}
\Vector{f}(a_{00} \Vector{v}_{00} + a_{01} \Vector{v}_{01}, \ldots, a_{k0} \Vector{v}_{k0} + a_{k1} \Vector{v}_{k1})
 =  \sum_{i_0 \ldots i_k = 0,1} (a_{0i_0} \ldots a_{ki_k}) \Vector{f}(\Vector{v}_{0i_0} \ldots \Vector{v}_{ki_k}).
\end{equation}

The derivative of $\Vector{f}$
at the point $(\Vector{v}_0 \ldots \Vector{v}_k)$, applied to the vector $(\Vector{u}_0 \ldots \Vector{u}_k)$ is

\begin{equation}
\Derivative{\Vector{f}}[(\Vector{v}_0 \ldots \Vector{v}_k)][\Vector{u}_0 \ldots \Vector{u}_k]
 =  \sum_{i=0,k} \Vector{f}(\Vector{v}_0 \ldots \Vector{v}_{i-1},\Vector{u}_i,\Vector{v}_{i+1} \ldots \Vector{v}_k).
\end{equation}

See Spivak \cite[ex.~2-14]{Spivak:1965:CalculusOnManifolds}.

\end{plSection}%{Multilinear functions}
%-----------------------------------------------------------------
\begin{plSection}{Bilinear functions}
\label{sec:Derivatives-of-bilinear-functions}

Bilinear functions are a useful special case of multilinear functions.

A function $\Vector{f}(\Vector{v},\Vector{u}):\Space{V}_0 \oplus \Space{V}_1 \mapsto \Space{W}$
is \textit{bilinear} if
\begin{eqnarray}
\Vector{f}(a_0 \Vector{v}_0 + a_1 \Vector{v}_1, b_0 \Vector{u}_0 + b_1 \Vector{u}_1)
& =  & a_0 b_0 f(\Vector{v}_0,\Vector{u}_0)
+  a_0 b_1 f(\Vector{v}_0,\Vector{u}_1)
\\
& +  & a_1 b_0 f(\Vector{v}_q,\Vector{u}_0)
 +  a_1 b_1 f(\Vector{v}_q,\Vector{u}_1).
\nonumber
\end{eqnarray}

The derivative of $\Vector{f}$
at the point $(\Vector{v}_0,\Vector{u}_0)$, 
applied to the vector $(\Vector{v},\Vector{u})$ is
\begin{equation}
\label{eq:bilinear-derivative}
\Derivative{\Vector{f}}
[(\Vector{v}_0,\Vector{u}_0)][\Vector{v},\Vector{u}]
 = \Vector{f}(\Vector{v}_0,\Vector{u}) 
 + \Vector{f}(\Vector{v},\Vector{u}_0).
\end{equation}

See \citeAuthorYearTitle[ex.~2-12]{Spivak:1965:CalculusOnManifolds}.

%-----------------------------------------------------------------
\begin{plSection}{Cross products}
\label{sec:Derivatives-of-cross-products}

We can view the 3-dimensional cross product
$ \times $
as a bilinear function
$\times(\Vector{v},\Vector{u}) 
= \Vector{v} \times \Vector{u} : \Reals^3 \oplus \Reals^3 
\mapsto \Reals^3$.
From \cref{eq:bilinear-derivative},
$\Derivative{\times}
[(\Vector{v}_0,\Vector{u}_0)]
[\Vector{v},\Vector{u}] 
= \Vector{v}_0 \times \Vector{u} 
+ \Vector{v} \times \Vector{u}_0$.

Suppose
$\Vector{f}:\Space{V} \mapsto \Reals^3$, and
$\Vector{g}:\Space{V} \mapsto \Reals^3$.
The derivative of $\Vector{f} \times \Vector{g}$ is:
\begin{eqnarray}
\Derivative{(\Vector{f} \times \Vector{g})}[\Vector{v}_0][\Vector{v}]
& =
& 
\Derivative{\times}
[\Vector{f}(\Vector{v}_0),\Vector{g}(\Vector{v}_0))]
\circ 
(\Derivative{\Vector{f}}[\Vector{v}_0][\Vector{v}], 
\Derivative{\Vector{g}}[\Vector{v}_0][\Vector{v}])
\\
& =
& \Vector{f}(\Vector{v}_0) 
\times 
\Derivative{\Vector{g}}[\Vector{v}_0][\Vector{v}]
 + 
 \Derivative{\Vector{f}}
 [\Vector{v}_0][\Vector{v}] 
 \times \Vector{g}(\Vector{v}_0) \nonumber
\end{eqnarray}

\end{plSection}%{Cross products}
%-----------------------------------------------------------------
\end{plSection}%{Bilinear functions}
%-----------------------------------------------------------------
\begin{plSection}{Scalar products}
\label{sec:Derivatives-of-scalar-products}

Suppose
$f:\Space{V} \mapsto \Reals$, and
$\Vector{g}:\Space{V} \mapsto \Space{W}$.
It follows from the chain rule that the derivative of $\Vector{h} = f\Vector{g}$ is:
\begin{eqnarray}
\label{eq:scalar_product_derivative}
\Derivative{(f\Vector{g})}[\Vector{v}] 
& =  &
f(\Vector{v}) \Derivative{\Vector{g}}[\Vector{v}] 
+ \Vector{g}(\Vector{v}) 
\Derivative{f}[\Vector{v}]
\\
& =  &
f(\Vector{v}) \Derivative{\Vector{g}}[\Vector{v}] 
+ \Vector{g}(\Vector{v}) \otimes 
\Gradient{f}[\Vector{v}]
\nonumber
\end{eqnarray}

\end{plSection}%{Scalar products}
%-----------------------------------------------------------------
\begin{plSection}{Normalized functions}
\label{sec:Derivatives-of-normalized-functionss}

Let $\tilde{\Vector{f}}$ be the normalized version of $\Vector{f}$:
$\tilde{\Vector{f}}  =  \frac{\Vector{f}}{\| \Vector{f} \|}$.
Then, from equations \ref{eq:scalar_product_derivative}
and \ref{eq:norm_derivative}:
\begin{eqnarray}
\Derivative{\tilde{\Vector{f}}}[\Vector{v}][\Vector{u}]
& = &
\Derivative{\left( \frac{\Vector{f}}{\| \Vector{f} \|}\right)}[\Vector{v}][\Vector{u}]
\\
& = &
\frac{\Derivative{\Vector{f}}[\Vector{v}][\Vector{u}]}{\| \Vector{f}(\Vector{v}) \|}
 +
\Vector{f}(\Vector{v})  \Derivative{ \left( \frac{1}{\| \Vector{f} \|} \right) }[\Vector{v}][\Vector{u}] \nonumber \\
& = &
\frac{
\Derivative{\Vector{f}}[\Vector{v}][\Vector{u}]
}
{\| \Vector{f}(\Vector{v}) \|}
 -
\Vector{f}(\Vector{v})
\frac{
\Derivative{\| \Vector{f} \|}[\Vector{v}][\Vector{u}]
}
{\|\Vector{f}(\Vector{v})\|^2} \nonumber \\
& = &
\frac{
\Derivative{\Vector{f}}[\Vector{v}][\Vector{u}]
}
{\| \Vector{f}(\Vector{v}) \| }
 -
\Vector{f}(\Vector{v}) 
\left(
 \frac{\Vector{f}(\Vector{v})^\dagger}
 {\| \Vector{f}(\Vector{v}) \|^3}  
 \Derivative{\Vector{f}}[\Vector{v}][\Vector{u}] 
 \right) \nonumber \\
& = &
\frac{
\| \Vector{f}(\Vector{v}) \|^2 
\Derivative{\Vector{f}}[\Vector{v}][\Vector{u}]
 -
\Vector{f}(\Vector{v})
\left( \Vector{f}(\Vector{v})
 \bullet 
 \Derivative{\Vector{f}}[\Vector{v}][\Vector{u}] \right)
}
{\| \Vector{f}(\Vector{v}) \|^3}  \nonumber \\
& = &
\frac{
\| \Vector{f}(\Vector{v}) \|^2 
\Identity_{\Space{W}} 
\,-\, 
\left( \Vector{f}(\Vector{v}) \otimes \Vector{f}(\Vector{v})
 \right)  
 }
{ 
\| \Vector{f}(\Vector{v}) \|^3 
}
\Derivative{\Vector{f}}[\Vector{v}][\Vector{u}] \nonumber \\
& = &
\frac{\Identity_{\Space{W}} 
\,-\, 
\left( 
\tilde{\Vector{f}}(\Vector{v})
 \otimes 
 \tilde{\Vector{f}}(\Vector{v}) \right)  
 }
{\| \Vector{f}(\Vector{v}) \|}
\Derivative{\Vector{f}}[\Vector{v}][\Vector{u}] \nonumber
\end{eqnarray}


We can write the derivative above without reference to the argument $\Vector{u}$:
\begin{equation}
\label{eq:normalized_function_derivative}
\Derivative{\tilde{\Vector{f}}}[\Vector{v}]
 =
\Derivative{\left( \frac{\Vector{f}}{\| \Vector{f} \|} \right)}[\Vector{v}]
 =
\frac{\Identity_{\Space{W}} - \left( \tilde{\Vector{f}}(\Vector{v}) \otimes \tilde{\Vector{f}}(\Vector{v}) \right) }
{ \| \Vector{f}(\Vector{v}) \| }
\Derivative{\Vector{f}}[\Vector{v}]
\end{equation}

A common, trivial, normalized function is the normalized version of
a vector: $\tilde{\Vector{v}} =  \frac{\Vector{v}}{ \| \Vector{v} \| }$.

From \cref{eq:normalized_function_derivative}
it follows that:
\begin{equation}
\label{eq:normalized_vector_derivative}
\Derivative{\tilde{\Vector{v}}}[\Vector{u}]
 =
\Derivative{ \left( \frac{\Vector{v}}{ \| \Vector{v} \| } \right) }[\Vector{u}]
 =
\frac{\Identity_{\Space{V}} - \left( \tilde{\Vector{u}} \otimes \tilde{\Vector{u}} \right) }
{ \| \Vector{u} \| }
 =
\frac{\| \Vector{u} \|^2 \Identity_{\Space{V}} - \left( \Vector{u} \otimes \Vector{u} \right) }
{\| \Vector{u} \|^3}
\end{equation}

\end{plSection}%{Normalized functions}
%-----------------------------------------------------------------
\end{plSection}%{Vector-valued functions}
%-----------------------------------------------------------------
\begin{plSection}{Real-valued functions}
\label{sec:derivatives-of-real-valued-functions}
%-----------------------------------------------------------------
\begin{plSection}{Inner products}
\label{sec:derivatives-of-inner-products}

We can view the inner product on $\Space{V}$, $\Vector{v} \bullet \Vector{u}$,
as a bilinear function $\bullet(\Vector{v},\Vector{u}) : \Space{V} \oplus \Space{V} \mapsto \Reals$.
Thus
\begin{equation}
\Derivative{\bullet}
[(\Vector{v}_0,\Vector{u}_0)]
[\Vector{v},\Vector{u}] 
= 
\Vector{v}_0 \bullet \Vector{u} + \Vector{v} \bullet \Vector{u}_0.
\end{equation}

Suppose
$\Vector{f}:\Space{V} \mapsto \Space{V}$, and
$\Vector{g}:\Space{V} \mapsto \Space{V}$.
The derivative of $\Vector{f} \bullet \Vector{g}$ is:
\begin{eqnarray}
\label{eq:dot_derivative}
\Derivative{(\Vector{f} \bullet \Vector{g})}[\Vector{v}_0][\Vector{v}]
& =
& \Derivative{\bullet}
[(\Vector{f}(\Vector{v}_0),\Vector{g}(\Vector{v}_0))]
\circ 
(\Derivative{\Vector{f}}{\Vector{v}_0}[\Vector{v}], 
\Derivative{\Vector{g}}[\Vector{v}_0][\Vector{v}])
\\
& =
& \Vector{f}(\Vector{v}_0) \bullet 
\Derivative{\Vector{g}}{\Vector{v}_0}{\Vector{v}}  
+  
\Vector{g}(\Vector{v}_0) \bullet 
\Derivative{\Vector{f}}[\Vector{v}_0][\Vector{v}] 
\nonumber
\end{eqnarray}

See \citeAuthorYearTitle[ex.~2-13]{Spivak:1965:CalculusOnManifolds}.
\end{plSection}%{Inner products}
%-----------------------------------------------------------------
\begin{plSection}{Angles}
\label{sec:derivatives-of-angles}

The angle between 2 vectors $\Vector{v}_0, \Vector{v}_1 \in \Space{V}$,
is the inverse cosine of their normalized inner product:
$\theta(\Vector{v}_0,\Vector{v}_1)
=
\cos^{-1} \left( \frac{ \Vector{v}_0 \bullet \Vector{v}_1 } {\|\Vector{v}_0\| \|\Vector{v}_1\|} \right)$.
Recall that the derivative of the $\cos^{-1}$ is
$\frac{\mathrm d}{\mathrm dx} \cos^{-1}(x) = \frac{-1}{\sqrt{1 - x^2} }$.
It follows that:
\begin{eqnarray*}
\Gradient[\Vector{v}_0]
{\theta(\Vector{v}_0,\Vector{v}_1)}
[\Vector{u}]
& = &
\frac{-1}
{ \sqrt{1 - \left( 
\frac{\Vector{u}_0 \bullet \Vector{u}_1}
{\| \Vector{u}_0 \| \| \Vector{u}_1 \|} 
\right)^2 }}
\Gradient[\Vector{v}_0]
{\left( \frac{\Vector{u}_0 \bullet \Vector{u}_1}
{\| \Vector{u}_0 \| \| \Vector{u}_1 \|} \right)}
[\Vector{u}]
\\
& = &
\frac{-\|\Vector{u}_0\|\|\Vector{u}_1\|}
{ \sqrt{\|\Vector{u}_0\|^2\|\Vector{u}_1\|^2 
- \left( \Vector{u}_0 \bullet \Vector{u}_1 \right)^2 }}
\left[
\frac{\Vector{u}_1}{\|\Vector{u}_0\|\|\Vector{u}_1\|}
+
\frac{\left( \Vector{u}_0 \bullet \Vector{u}_1 \right)}
{\| \Vector{u}_1 \|}
\Gradient[\Vector{v}_0]
{\left( \frac{1}{\| \Vector{v}_0 \|} \right)}
[\Vector{u}]
\right]
\nonumber
\\
& = &
\frac{-\|\Vector{u}_0\|\|\Vector{u}_1\|}
{ \sqrt{\|\Vector{u}_0\|^2\|\Vector{u}_1\|^2 
- \left( \Vector{u}_0 \bullet \Vector{u}_1 \right)^2 }}
\left[
\frac{\Vector{u}_1}{\|\Vector{u}_0\|\|\Vector{u}_1\|}
-
\frac{
\left( \Vector{u}_0 \bullet \Vector{u}_1 \right) \Vector{u}_0}
{\| \Vector{u}_1 \| \|\Vector{u}_0\|^3}
\right]
\nonumber
\\
& = &
\frac{-1}
{ \sqrt{
\|\Vector{u}_0\|^2\|\Vector{u}_1\|^2 
- \left( \Vector{u}_0 \bullet \Vector{u}_1 \right)^2 }}
\left[
\Vector{u}_1
-
\frac{
\left( \Vector{u}_0 \bullet \Vector{u}_1 \right) 
\Vector{u}_0}
{\|\Vector{u}_0\|^2}
\right]
\nonumber
\end{eqnarray*}
which results in
\begin{eqnarray}
\label{eq:angle_gradient}
\Gradient[\Vector{v}_0]
{\theta(\Vector{v}_0,\Vector{v}_1)}
[\Vector{u}]
& = &
\frac{- \Vector{u}_1 \perp \Vector{u}_0}
{ \sqrt{\|\Vector{u}_0\|^2\|\Vector{u}_1\|^2 
- \left( \Vector{u}_0 \bullet \Vector{u}_1 \right)^2 }}
\\
\Gradient[\Vector{v}_1]
{\theta(\Vector{v}_0,\Vector{v}_1)}
[\Vector{u}]
& = &
\frac{- \Vector{u}_0 \perp \Vector{u}_1}
{ \sqrt{\|\Vector{u}_0\|^2\|\Vector{u}_1\|^2 
- \left( \Vector{u}_0 \bullet \Vector{u}_1 \right)^2 }}
\nonumber
\end{eqnarray}

\end{plSection}%{Angles}
%-----------------------------------------------------------------
\begin{plSection}{Euclidean norm}
\label{sec:derivatives-of-euclidean-norm}

Let 
$l_2(\Vector{v}) = \| \Vector{v} \|: \Space{V} \mapsto \Reals$
be the usual euclidean norm on $\Space{V}$.
Let $l_2^2(\Vector{v}) = \| \Vector{v}  \|^2 $
be its square and $ \| \Vector{v}  \|^3$ the cube.
\begin{eqnarray}
\label{eq:l2-gradient}
\Gradient{l_2}[\Vector{v}] = \frac{\Vector{v}}{\|\Vector{v}\|} 
&
\Gradient{l_2^2}[\Vector{v}] = 2\Vector{v}
&
\Gradient{l_2^3}[\Vector{v}] = 3 \|\Vector{v}\| \Vector{v} 
\\
\Derivative{l_2}[\Vector{v}] 
= \frac{ \Vector{v}^\dagger }{ \| \Vector{v}  \|} 
&
\Derivative{l_2^2}[\Vector{v}] = 2\Vector{v}^\dagger 
&
\Derivative{l_2^3}[\Vector{v}] 
= 3 \| \Vector{v}  \| \Vector{v}^\dagger 
\nonumber
\end{eqnarray}

Let $\Vector{f}(\Vector{v}) : \Space{V} \mapsto \Space{W}$.
By the chain rule:
$\Derivative{\| \Vector{f} \|^2}{\Vector{v}}  
=  2 {\Vector{f}(\Vector{v})}^{\dagger} 
\Derivative{\Vector{f}}[\Vector{v}] $
and
$\Gradient{\| \Vector{f} \|^2}[\Vector{v}]  
=  2 \Derivative{\Vector{f}}[\Vector{v}]^\dagger 
\circ \Vector{f}(\Vector{v})$.
\begin{eqnarray}
\label{eq:norm_derivative}
\Derivative{\| \Vector{f} \|}[\Vector{v}]
& = &
\frac{\Vector{f}(\Vector{v})^\dagger}
{\| \Vector{f}(\Vector{v}) \|} 
\Derivative{\Vector{f}}[\Vector{v}]  \\
\Gradient{\| \Vector{f} \|}[\Vector{v}]
& = &
\left(
\Derivative{\Vector{f}}{\Vector{v}}\right)^\dagger 
\circ 
\frac{\Vector{f}(\Vector{v})}{\| \Vector{f}(\Vector{v}) \|}
\label{eq:norm_gradient}
\end{eqnarray}

\end{plSection}%{Euclidean norm}
%-----------------------------------------------------------------
\begin{plSection}{Canonical vector 'volume'}
\label{sec:Derivative-of-canonical-vector-volume}

Let $\text{volume} : \times^{n} \Space{R} \mapsto \Space{R}$ 
be defined as
\begin{equation}
\text{volume} \left( x_0  \ldots  x_{n-1} \right) = \prod_{i=0}^{n-1} x_i
\end{equation}
This is multilinear as a functional on $\times^{n} \Space{R}$.

We can also interpret this as a multilinear function on the
inner product space
$\oplus^{n} \Space{R}$.
In that case, $\text{volume}$ is the volume of the coordinate axis aligned
$n$-rectangle whose diagonal is $\Vector(x)$.

Note the dependence on both the choice of inner product, 
and the coordinate system.

\begin{equation}
\Derivative{\text{volume}}[(\Vector{v}_0 \ldots \Vector{v}_{n-1})][\Vector{u}_0 \ldots \Vector{u}_{n-1}]
 =  \sum_{i=0}^{n-1} \Vector{u}_i \left( \prod_{j \neq i} \Vector{v}_j \right).
\end{equation}

\end{plSection}%{Canonical vector 'volume'}
%-----------------------------------------------------------------
\end{plSection}%{Real-valued functions}
%-----------------------------------------------------------------
\begin{plSection}{Linear-function-valued functions}
\label{sec:Derivatives-of-linear-function-valued-functions}

The set of linear functions between two inner product spaces
$\left\{ \Vector{L} : \Space{V} \mapsto \Space{W} \right\}$
is itself a inner product space $\Space{L}(\Space{V},\Space{W})$,
with the inner product defined by
$\Vector{L} \bullet \Vector{M} = \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} \Vector{L}_{ij} \Vector{M}_{ij}$.
The set of linear functions
$\Vector{E}_{ij}^{\Space{L}(\Space{V},\Space{W})}  = \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}}$
are the canonical basis vectors for $\Space{L}(\Space{V},\Space{W})$.

If $\Vector{f}$ is a function between spaces of linear functions,
$\Vector{f} : \Space{L}(\Space{V}_0,\Space{W}_0) \mapsto \Space{L}(\Space{V}_1,\Space{W}_1)$,
its derivative, $\Derivative{\Vector{f}}$,
is a function from a space of linear functions
to a space of linear functions between two
spaces of linear functions:
$\Derivative{\Vector{f}} : \Space{L}(\Space{V}_0,\Space{W}_0) \mapsto
\Space{L}(\Space{L}(\Space{V}_0,\Space{W}_0), \Space{L}(\Space{V}_1,\Space{W}_1))$.
This can get a little confusing,
and it often helps to consider both the partial derivatives of $\Vector{f}$
and the gradients of the coordinates of $\Vector{f}$,
which can make it easier to apply the chain rule to
compositions of functions of functions via \cref{eq:chain-rule_partials}.

$\Partial{ij}{\Vector{f}}$ is the partial derivative with respect to its $ij$-th matrix coordinate,
that is, the directional derivative of $\Vector{f}$ in the direction
of the $ij$-th canonical basis vector, $\Vector{E}_{ij}^{\Space{L}(\Space{V}_0,\Space{W}_0)}$.
As usual the value of the partial derivative at a specific
$\Vector{L}_0 \in  \Space{L}(\Space{V}_0,\Space{W}_0)$,
$\Partial{ij}{\Vector{f}}[\Vector{L}_0]$ 
is an element of the co-domain of $\Vector{f}$,
a linear function in  $\Space{L}(\Space{V}_1,\Space{W}_1)$.

$\Gradient{\Vector{f}_{kl}}$ is the gradient of the $kl$-th matrix coordinate of the value of $\Vector{f}$.
As usual, the value of the gradient at a specific $\Vector{L}_0$,
$\Gradient{\Vector{f}_{kl}}[\Vector{L}_0]$ 
is an element of the domain of $\Vector{f}$,
a linear function in $\Space{L}(\Space{V}_0,\Space{W}_0)$.

Note that nether of these are elements of the Jacobian of $\Vector{f}$,
which needs 4 indexes: $\Partial{ij}{\Vector{f}_{kl}}$.

I am particularly interested in computing the derivative of the
pseudo-inverse: $\Pseudoinverse(\Vector{L}) \equiv \Vector{L}^{-}$.
The set of full rank linear functions is an open set,
and we can define the derivative of $\Pseudoinverse(\Vector{L})$ there.
For full rank linear functions,
we can use the chain rule and the identity
$\Vector{L}^{-} = \left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1} \Vector{L}^{\dagger}$
(\cref{eq:full-rank-pseudo-inverse})
to compute the derivative of the pseudo-inverse
(\cref{sec:Derivative-of-pseudo-inverse}).

To do this I will first establish partial derivatives and gradients of:
\begin{equation}
\begin{aligned}
\label{eq:transpose-derivative}
&\Transpose(\Vector{L}) \equiv \Vector{L}^{\dagger}
&&\Partial{ij}{\Transpose}[\Vector{L}]
 =  \Vector{e}_j^{\Space{V}} \otimes \Vector{e}_i^{\Space{W}}
\forall \Vector{L}
\\
&\Vector{h}( \Vector{L} ) = \Vector{f} ( \Vector{L} ) \Vector{g} ( \Vector{L} )
&&\text{Section~\ref{sec:Derivatives-of-function-products} }
\\
&\LTL(\Vector{L}) \equiv \Vector{L}^{\dagger} \Vector{L}
&&\text{Section~\ref{sec:Derivatives-of-LTL} }
\\
&\Inverse(\Vector{L}) \equiv \Vector{L}^{-1}
&&\text{Section~\ref{sec:Derivative-of-inverse} }
\end{aligned}
\end{equation}

%-----------------------------------------------------------------
\begin{plSection}{Function products}
\label{sec:Derivatives-of-function-products}

Let
$\Vector{f} : \Space{L}(\Space{V}_0,\Space{W}_0) \mapsto \Space{L}(\Space{V}_1,\Space{W}_1)$,
$\Vector{g} : \Space{L}(\Space{V}_0,\Space{W}_0) \mapsto \Space{L}(\Space{U}_1,\Space{V}_1)$,
and
$\Vector{h} = \Vector{f}\Vector{g} : \Space{L}(\Space{V}_0,\Space{W}_0) \mapsto \Space{L}(\Space{U}_1,\Space{W}_1)$.
Note that
$\Partial{ij}{\Vector{f}}[\Vector{L}] 
\in  \Space{L}(\Space{V}_1,\Space{W}_1)$,
$\Partial{ij}{\Vector{g}}[\Vector{L}] 
\in  \Space{L}(\Space{U}_1,\Space{V}_1)$,
and
$\Partial{ij}{\Vector{h}}[\Vector{L}] 
\in  \Space{L}(\Space{U}_1,\Space{W}_1)$.
Consider the matrix representation of 
$\Partial{ij}{\Vector{h}}[\Vector{L}]$:
\begin{eqnarray}
\left( 
\Partial{ij}{\Vector{h}}[\Vector{L}] 
\right)_{kl}
& = &
\Partial{ij}{\Vector{h}_{kl}}[\Vector{L}]
\\
& = &
\Partial{ij}{\left( \sum_{m} \Vector{f}_{km}\Vector{g}_{ml} \right)}[\Vector{L}]
\nonumber
\\
& = &
\sum_{m}  \left[
\left( 
\Partial{ij}{\Vector{f}_{km}}[\Vector{L}]
 \right) \Vector{g}_{ml}(\Vector{L})
+
\Vector{f}_{km}(\Vector{L}) 
\left( 
\Partial{ij}{\Vector{g}_{ml}}[\Vector{L}] 
\right)
\right]
\nonumber
\\
& = &
\left[
\left( 
\Partial{ij}{\Vector{f}}[\Vector{L}] 
\right) \Vector{g}(\Vector{L})
+
\Vector{f}(\Vector{L}) \left(
\Partial{ij}{\Vector{g}}[\Vector{L}] 
\right)
\right]_{kl}
\nonumber
\end{eqnarray}
Therefore
\begin{equation}
\label{eq:function-product-derivative}
\Partial{ij}{\Vector{h}}[\Vector{L}]
 =
\left( 
\Partial{ij}{\Vector{f}}[\Vector{L}] 
\right) \Vector{g}(\Vector{L})
+
\Vector{f}(\Vector{L}) \left( 
\Partial{ij}{\Vector{g}}[\Vector{L}] 
\right)
\end{equation}

\end{plSection}%{Function products}
%-----------------------------------------------------------------
\begin{plSection}{\texorpdfstring{$\Vector{L}^{\dagger} \Vector{L}$}{LTL}}
\label{sec:Derivatives-of-LTL}

A simple function on linear functions
is $\LTL(\Vector{L}) \equiv \Vector{L}^{\dagger} \Vector{L}
: \Space{L}(\Space{V},\Space{W}) \mapsto \Space{L}(\Space{V},\Space{V})$.

The partial derivative is computed using equations
\ref{eq:transpose-derivative}
and
\ref{eq:function-product-derivative}:

\begin{equation}
\Partial{ij}{\LTL}[\Vector{L}]
=
\left( \Vector{e}_j^{\Space{V}} \otimes \Vector{e}_i^{\Space{W}} \right) \Vector{L}
+
\Vector{L}^{\dagger} \left( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} \right)
=
\left( \Vector{e}_j^{\Space{V}} \otimes \Vector{r}_i^{\Vector{L}} \right)
+
\left( \Vector{r}_i^{\Vector{L}} \otimes \Vector{e}_j^{\Space{V}} \right)
\end{equation}
where $\Vector{r}_i^{\Vector{L}} \in \Space{V}$ is the $i$th 'row' of $\Vector{L}$
in the representation $\Vector{L} = \sum_{i=0}^{m-1} \Vector{e}_i^{\Space{W}} \otimes \Vector{r}_i^{\Vector{L}}$.

The Jacobian, which has 4 indexes here, is given by:
\begin{equation}
\Partial{ij}{\LTL_{kl}}[\Vector{L}]
 =
\left( 
\Partial{ij}{\LTL}[\Vector{L}] 
\right)_{kl}
=
\delta_{jl} \Vector{L}_{ik}
+
\delta_{jk} \Vector{L}_{il}
\end{equation}
where, as usual, $\delta_{ij} = 1$ if $i=j$ and  $0$ if $i \neq j$.
From the Jacobian, we can compute the gradients of $\LTL_{kl}$
using \cref{eq:gradient-from-partials}
and the fact that
$\Vector{E}_{ij}^{\Space{L}(\Space{V},\Space{W})}  = \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}}$
are the canonical basis vectors for $\Space{L}(\Space{V},\Space{W})$:
\begin{eqnarray}
\Gradient{\LTL_{kl}}[\Vector{L}]
& = &
\sum_{ij}
\left( 
\Partial{ij}{\LTL_{kl}}[\Vector{L}] 
\right)
\left( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} \right)
\\
& = &
\sum_{ij}
\left( \delta_{jl} \Vector{L}_{ik} + \delta_{jk} \Vector{L}_{il} \right)
\left( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} \right)
\nonumber
\\
& = &
\sum_{i}
\left(
\Vector{L}_{ik}  \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_l^{\Space{V}}
\right)
+
\sum_{i}
\left(
\Vector{L}_{il}  \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_k^{\Space{V}}
\right)
\nonumber
\\
& = &
\left(
\Vector{c}_k^{\Vector{L}} \otimes \Vector{e}_l^{\Space{V}}
\right)
+
\left(
\Vector{c}_l^{\Vector{L}} \otimes \Vector{e}_k^{\Space{V}}
\right)
\nonumber
\end{eqnarray}
where $\Vector{c}_j^{\Vector{L}} \in \Space{W}$ is the $j$th 'column' of $\Vector{L}$
in the representation
$\Vector{L} = \sum_{j=0}^{n-1} \Vector{c}_j^{\Vector{L}} \otimes \Vector{e}_j^{\Space{V}}$.

\end{plSection}%{\texorpdfstring{$\Vector{L}^{\dagger} \Vector{L}$}{LTL}}
%-----------------------------------------------------------------
\begin{plSection}{Inverse}
\label{sec:Derivative-of-inverse}

$\Inverse()$ here is interpreted in the traditional sense:
$\Vector{L}^{-1}(\Vector{w}) = \Vector{v}$ if there exists a unique $\Vector{v}$ such that $\Vector{w} = \Vector{L}(\Vector{v})$,
and is either considered undefined, or assigned an arbitrary
value, such as $\Vector{0}$, otherwise.
A function $\Vector{L} : \Space{V} \mapsto \Space{W}$ is \textit{invertible}
if, for all $\Vector{w} \in \Space{W}$, there exists a $\Vector{v}$ such that
$\Vector{w} = \Vector{L} \Vector{v}$.
In any reasonable topology,
the set of invertible linear functions $\Space{V} \mapsto \Space{W}$
is an open subset of the set of all linear functions,
and $\Inverse()$ is continuous and differentiable there.

The partial derivative is the value of the following, when the limit exists:
\begin{displaymath}
\Partial{ij}{\Inverse()}[\Vector{L}]
 =
\lim_{ h \mapsto 0}
\frac{ \left( \Vector{L} + h (\Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}}) \right)^{-1} - \Vector{L}^{-1} }{h}
\end{displaymath}
Note that
\begin{displaymath}
\Vector{L} + h (\Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}})
 =
\left( \Identity^{\Space{W}} - ( -h ( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} )) \Vector{L}^{-1} \right) \Vector{L}
\end{displaymath}
and
\begin{eqnarray*}
\left( \Vector{L} + h (\Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}}) \right)^{-1}
& = &
\Vector{L}^{-1} \left( \Identity^{\Space{W}} - ( -h )( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} ) \Vector{L}^{-1} \right)^{-1}
\\
& = &
\Vector{L}^{-1} \sum_{k=0}^{\infty} \left( -h ( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} ) \Vector{L}^{-1} \right)^{k}
\nonumber
\end{eqnarray*}
Therefore
\begin{displaymath}
\frac{ \left( \Vector{L} + h (\Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}}) \right)^{-1} - \Vector{L}^{-1} }{h}
 =
- \Vector{L}^{-1} ( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} )  \Vector{L}^{-1} + O(h)
\end{displaymath}
which implies
\begin{equation}
\Partial{ij}{\Vector{L}^{-1}}
 =
- \left[
\Vector{L}^{-1}
\left( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} \right)
\Vector{L}^{-1}
\right]
\end{equation}

\newgeometry{onecolumn=true}

\end{plSection}%{Inverse}
%-----------------------------------------------------------------
\begin{plSection}{Pseudoinverse}
\label{sec:Derivative-of-pseudoinverse}

$\Pseudoinverse(\Vector{L}) \equiv \Vector{L}^{-}$

If $\kernel(\Vector{L}) = \Vector{0}$, $\Vector{L}$ is said to have \textit{full rank}.
The set of full rank linear functions is an open set,
and we can define the derivative of $\Pseudoinverse(\Vector{L})$ there.
For a full rank function,
$\Vector{L}^{-} = \left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1} \Vector{L}^{\dagger}$
(see \cref{eq:full-rank-pseudo-inverse}).
It follows from \cref{eq:function-product-derivative} that
\begin{eqnarray}
\Partial{ij}{\Pseudoinverse}[\Vector{L}]
& = &
\Partial{ij}{\Inverse(\LTL())\Transpose()}[\Vector{L}]
\\
& = &
\left[
\left( 
\Partial{ij}{\Inverse(\LTL())}[\Vector{L}] 
\right)
\Vector{L}^{\dagger}
\right]
+
\left[
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\Partial{ij}{\Transpose()}[\Vector{L}]
\right]
\nonumber
\\
& = &
\left[
\left( 
\Partial{ij}{\Inverse(\LTL())}[\Vector{L}] 
\right)
\Vector{L}^{\dagger}
\right]
+
\left[
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\left( \Vector{e}_j^{\Space{V}} \otimes \Vector{e}_i^{\Space{W}} \right)
\right]
\nonumber
\end{eqnarray}

By the chain rule
\begin{eqnarray}
\Derivative{\Inverse(\LTL())}[\Vector{L}]
& = &
\sum_{kl}
\Partial{kl}{\Inverse}[\Vector{L}^{\dagger}\Vector{L}]
\otimes
\Gradient{\LTL_{kl}}[\Vector{L}]
\\
& = &
\sum_{kl}
- \left[
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\left( \Vector{e}_k^{\Space{V}} \otimes \Vector{e}_l^{\Space{V}} \right)
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\right]
\otimes
\left[
\left( \Vector{c}_k^{\Vector{L}} \otimes \Vector{e}_l^{\Space{V}} \right)
+
\left( \Vector{c}_l^{\Vector{L}} \otimes \Vector{e}_k^{\Space{V}} \right)
\right]
\nonumber
\end{eqnarray}

To minimize confusion,
recall that $\Derivative{\Inverse(\LTL())}[\Vector{L}]$ is
a linear function from $\Space{L}(\Space{V},\Space{W}) \mapsto \Space{L}(\Space{V},\Space{V})$.
Note that the central tensor product ($\otimes$) above
is a product of
$
- \left[
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\left( \Vector{e}_k^{\Space{V}} \otimes \Vector{e}_l^{\Space{V}} \right)
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\right]
$,
an element of $\Space{L}(\Space{V},\Space{V})$
and
$
\left[
\left( \Vector{c}_k^{\Vector{L}} \otimes \Vector{e}_l^{\Space{V}} \right)
+
\left( \Vector{c}_l^{\Vector{L}} \otimes \Vector{e}_k^{\Space{V}} \right)
\right]
$,
an element of $\Space{L}(\Space{V},\Space{W})$.

It follows from \cref{eq:partial-full-dervatives} that
\begin{eqnarray}
\Partial{ij}{\Inverse(\LTL())}[\Vector{L}]
& = &
\Derivative{\Inverse(\LTL())}[\Vector{L}]
\left( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} \right)
\\
& = &
\sum_{kl}
- \left[
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\left( \Vector{e}_k^{\Space{V}} \otimes \Vector{e}_l^{\Space{V}} \right)
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\right]
\otimes
\left[
\left( \Vector{c}_k^{\Vector{L}} \otimes \Vector{e}_l^{\Space{V}} \right)
+
\left( \Vector{c}_l^{\Vector{L}} \otimes \Vector{e}_k^{\Space{V}} \right)
\right]
\left( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} \right)
\nonumber
\\
& = &
\sum_{kl}
- \left[
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\left( \Vector{e}_k^{\Space{V}} \otimes \Vector{e}_l^{\Space{V}} \right)
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\right]
\left[
\delta_{jl}
\Vector{L}_{ik}
+
\delta_{jk}
\Vector{L}_{il}
\right]
\nonumber
\\
& = &
-
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\left[
\sum_{k}
\Vector{L}_{ik}
\left(
\left( \Vector{e}_k^{\Space{V}} \otimes \Vector{e}_j^{\Space{V}} \right)
+
\left( \Vector{e}_j^{\Space{V}} \otimes \Vector{e}_k^{\Space{V}} \right)
\right)
\right]
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\nonumber
\\
& = &
-
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\left[
\left( \Vector{r}_i^{\Vector{L}} \otimes \Vector{e}_j^{\Space{V}} \right)
+
\left( \Vector{e}_j^{\Space{V}} \otimes \Vector{r}_i^{\Vector{L}} \right)
\right]
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\nonumber
\end{eqnarray}

Putting it all together:
\begin{eqnarray}
\Partial{ij}{\Pseudoinverse}[\Vector{L}]
& = &
\left[
-
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\left[
\left( \Vector{r}_i^{\Vector{L}} \otimes \Vector{e}_j^{\Space{V}} \right)
+
\left( \Vector{e}_j^{\Space{V}} \otimes \Vector{r}_i^{\Vector{L}} \right)
\right]
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\Vector{L}^{\dagger}
\right]
+
\left[
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\left( \Vector{e}_j^{\Space{V}} \otimes \Vector{e}_i^{\Space{W}} \right)
\right]
\nonumber
\\
& = &
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\left[
\left( \Vector{e}_j^{\Space{V}} \otimes \Vector{e}_i^{\Space{W}} \right)
-
\left(
\left[
\left( \Vector{r}_i^{\Vector{L}} \otimes \Vector{e}_j^{\Space{V}} \right)
+
\left( \Vector{e}_j^{\Space{V}} \otimes \Vector{r}_i^{\Vector{L}} \right)
\right]
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\Vector{L}^{\dagger}
\right)
\right]
\nonumber
\\
& = &
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1}
\left[
\left( \Vector{e}_j^{\Space{V}} \otimes \Vector{e}_i^{\Space{W}} \right)
-
\left(
\left( \Vector{r}_i^{\Vector{L}} \otimes \Vector{e}_j^{\Space{V}} \right)
+
\left( \Vector{e}_j^{\Space{V}} \otimes \Vector{r}_i^{\Vector{L}} \right)
\right)
\Vector{L}^{-}
\right]
\end{eqnarray}

\restoregeometry

\end{plSection}%{Pseudoinverse}
%-----------------------------------------------------------------
\end{plSection}%{Linear-function-valued functions}
%-----------------------------------------------------------------
\end{plSection}%{Derivatives}
